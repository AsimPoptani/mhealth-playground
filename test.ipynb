{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 5  5  5  5  5  5  5  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  3\n  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  6  6  6  6  6  6  6  6  7\n  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9 10\n 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12  5\n  5  1  1  2  2  3  3  4  4  6  6  7  7  8  8  9  9 10 10 11 11 12 12]\n12\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 1. 0.]\n [0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 1.]]\nWARNING:tensorflow:From C:\\Apps\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n[[-9.4367e+00  1.1639e-01 -8.7374e-01 ...  1.7785e+01 -3.4744e+01\n   6.4241e+01]\n [-4.8660e+00  3.7209e-01 -6.6125e-01 ... -6.0199e-01 -2.4292e+01\n   5.6549e+01]\n [-9.7842e+00 -6.8761e-01  3.9075e-01 ...  8.9946e-01  1.7988e-01\n   7.3821e-01]\n ...\n [-2.1892e+01  1.1818e+00 -1.5830e+01 ...  7.2210e+01  5.7606e+01\n  -6.5558e+01]\n [-2.7137e+00 -1.1825e+00 -3.6538e+00 ... -1.3272e+01  1.9272e+01\n  -1.3692e+02]\n [-2.1772e+01 -3.3342e+00 -5.5211e+00 ... -1.9838e+01 -8.0757e+01\n   8.9580e+01]]\n[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
    }
   ],
   "source": [
    "# I tried removing 0 from classification seems to improve accuracy as expected 1.0. validation accuracy a little bit\n",
    "# but I haven't fully removed 13 and reduced to 12 classifications.\n",
    "# For some reason the one hot or the softmax model bit dont like it when I change the values form 13 to 12\n",
    "\n",
    "from helper_functions import mhealth_get_dataset\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Hyperparameters\n",
    "# This is how many samples per col\n",
    "data_length = 2\n",
    "\n",
    "# Prep data\n",
    "\n",
    "\n",
    "# Get the dataset\n",
    "dataset=mhealth_get_dataset()\n",
    "\n",
    "# shuffle dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "\n",
    "# training_users,test_data = dataset[:6], dataset[6:]\n",
    "# training_users,test_data = dataset[:8], dataset[8:] # get 8 training users and 2 test users\n",
    "training_users,test_data = dataset[:8], dataset[8:]\n",
    "\n",
    "\n",
    "# Remove all records with a certain value from the list of lists aka the 2D list under the 'data' key in each user dictionary in the list of user information dictionaries.\n",
    "def remove_records_by_val(lst_of_users_dicos, val=0):\n",
    "    for i in range(len(lst_of_users_dicos)):\n",
    "        lst_of_users_dicos[i]['data'] = [lst for lst in lst_of_users_dicos[i]['data'] if lst[23] != val]\n",
    "\n",
    "#Same thing just not as a function\n",
    "\n",
    "# for user in training_users:\n",
    "# for i in range(len(training_users)):\n",
    "#     training_users[i]['data'] = [lst for lst in training_users[i]['data'] if lst[23] != 0]\n",
    "\n",
    "# # for user in training_users:\n",
    "# for i in range(len(test_data)):\n",
    "#     test_data[i]['data'] = [lst for lst in test_data[i]['data'] if lst[23] != 0]\n",
    "\n",
    "\n",
    "# In this case removing 0 to see affect on validation accuracy\n",
    "# The 0 classification (no activity) is very... unpredictable and the network is most certainly having difficulty with this and other activities\n",
    "remove_records_by_val(training_users, 0)\n",
    "remove_records_by_val(test_data, 0)\n",
    "\n",
    "\n",
    "\n",
    "training_data_pre  = defaultdict(list) \n",
    "test_data_pre =defaultdict(list)\n",
    "\n",
    "counter=0\n",
    "previous_label=None\n",
    "to_put=[]\n",
    "for user in training_users:\n",
    "    for user_data in user['data']:\n",
    "        if not previous_label==user_data[23]:\n",
    "            counter=1\n",
    "            to_put=[]\n",
    "            previous_label=user_data[23]\n",
    "            user_data.pop()\n",
    "            to_put+=user_data\n",
    "        elif previous_label==user_data[23]:\n",
    "            counter+=1\n",
    "            user_data.pop()\n",
    "            to_put+=user_data\n",
    "        if counter == data_length:\n",
    "            training_data_pre[previous_label].append(to_put)\n",
    "            to_put=[]\n",
    "\n",
    "counter=0\n",
    "previous_label=None\n",
    "to_put=[]\n",
    "for user in test_data:\n",
    "    for user_data in user['data']:\n",
    "        if not previous_label==user_data[23]:\n",
    "            counter=1\n",
    "            to_put=[]\n",
    "            previous_label=user_data[23]\n",
    "            user_data.pop()\n",
    "            to_put+=user_data\n",
    "        elif previous_label==user_data[23]:\n",
    "            counter+=1\n",
    "            user_data.pop()\n",
    "            to_put+=user_data\n",
    "        if counter == data_length:\n",
    "            test_data_pre[previous_label].append(to_put)\n",
    "            to_put=[]\n",
    "\n",
    "training_data,training_labels=[],[]\n",
    "test_data,test_labels=[],[]\n",
    "\n",
    "def get_one_hot(targets, nb_classes): # 0 taken out means onehot is wrong. 1-12\n",
    "    print(targets)\n",
    "    print(nb_classes)\n",
    "    res = np.eye(nb_classes)[np.array(targets - 1).reshape(-1)]\n",
    "    # res = np.eye(nb_classes)[np.array(targets)]\n",
    "    print(res)\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "for training_label,training in training_data_pre.items():\n",
    "    for train in training:\n",
    "        training_labels.append(int(training_label))\n",
    "        training_data.append(train)\n",
    "\n",
    "training_data=np.array(training_data)\n",
    "\n",
    "\n",
    "for training_label,training in test_data_pre.items():\n",
    "    for train in training:\n",
    "        test_labels.append(int(training_label))\n",
    "        test_data.append(train)\n",
    "\n",
    "test_data=np.array(test_data)\n",
    "\n",
    "training_labels_len=len(training_labels)\n",
    "labels=training_labels+test_labels\n",
    "\n",
    "# removing 0 to see affect on validation accuracy\n",
    "# labels=get_one_hot(np.array(labels),13)\n",
    "labels=get_one_hot(np.array(labels),12)\n",
    "\n",
    "training_labels=labels[:training_labels_len]\n",
    "test_labels=labels[training_labels_len:]\n",
    "\n",
    "# Okay lets create our model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(data_length*23), # Our input layer\n",
    "    tf.keras.layers.Dense(2000,activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(2000),\n",
    "\n",
    "    # tf.keras.layers.Dense(13,activation=tf.keras.activations.softmax) # Our output layer we have 13 classifcations # removing 0 to see affect on validation accuracy\n",
    "    tf.keras.layers.Dense(12,activation=tf.keras.activations.softmax) # Without 0\n",
    "\n",
    "])\n",
    "# Compile our model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "print(test_data)\n",
    "\n",
    "print(test_labels)\n",
    "\n",
    "# model.fit(training_data,training_labels,validation_data=(test_data,test_labels),epochs=200)#100\n",
    "# -9.2305,3.9479,-4.0236,0.10884,-0.058608,3.4836,-7.467,-8.0885,0.24675,-0.78799,-0.21807,0.19791,18.714,5.4366,2.2399,-5.5648,7.5238,0.47255,-0.25051,1.0172,9.3079,-2.787,-15.309,9\n",
    "# -8.6677,3.5852,-4.5615,-0.054422,0.22606,-0.75304,-7.1983,-5.9716,0.24675,-0.78799,-0.21807,0.25394,7.7039,9.586,-0.92224,-5.7833,9.6692,0.47255,-0.25051,1.0172,4.6822,1.4321,-15.444,9\n",
    "\n",
    "# test_array = np.array([[-9.2305,3.9479,-4.0236,0.10884,-0.058608,3.4836,-7.467,-8.0885,0.24675,-0.78799,-0.21807,0.19791,18.714,5.4366,2.2399,-5.5648,7.5238,0.47255,-0.25051,1.0172,9.3079,-2.787,-15.309,-8.6677,3.5852,-4.5615,-0.054422,0.22606,-0.75304,-7.1983,-5.9716,0.24675,-0.78799,-0.21807,0.25394,7.7039,9.586,-0.92224,-5.7833,9.6692,0.47255,-0.25051,1.0172,4.6822,1.4321,-15.444]])\n",
    "\n",
    "# print(model.predict([test_array])[0][8]*100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# With 0 \n",
    "\n",
    "# Epoch 100/100\n",
    "\n",
    "# training_users,test_data = dataset[:6], dataset[6:] 6 training users 4 test\n",
    "#157/157 [==============================] - 0s 319us/sample - loss: 0.1574 - acc: 0.9108 - val_loss: 10.5625 - val_acc: 0.3402\n",
    "\n",
    "\n",
    "#   Without 0_______________________________________________________________________________________________________________________\n",
    "\n",
    "# 6 training users 4 test\n",
    "# Epoch 100/100\n",
    "# 71/71 [==============================] - 0s 479us/sample - loss: 0.0053 - acc: 1.0000 - val_loss: 21.2243 - val_acc: 0.5217\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Dense(data_length*23), # Our input layer\n",
    "#     tf.keras.layers.Dense(500,activation=tf.keras.activations.relu),\n",
    "#     tf.keras.layers.Dense(500),\n",
    "#     tf.keras.layers.Dense(12,activation=tf.keras.activations.softmax) # Without 0\n",
    "# ])\n",
    "\n",
    "# 8 training users 2 test\n",
    "#__________________________\n",
    "\n",
    "# With Denses 500 in middle\n",
    "# Epoch 200/200\n",
    "# 95/95 [==============================] - 0s 790us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 7.4909 - val_acc: 0.5833\n",
    "\n",
    "# With Denses in the middle 1000 instead of 500\n",
    "# Epoch 200/200\n",
    "# 94/94 [==============================] - 0s 373us/sample - loss: 4.2624e-04 - acc: 1.0000 - val_loss: 20.9304 - val_acc: 0.5833\n",
    "\n",
    "\n",
    "# With Denses in the middle 2000 instead of 500\n",
    "# Epoch 200/200\n",
    "# 95/95 [==============================] - 0s 717us/sample - loss: 2.9468e-04 - acc: 1.0000 - val_loss: 42.0160 - val_acc: 0.6250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}