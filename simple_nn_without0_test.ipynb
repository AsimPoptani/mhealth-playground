{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 5  5  5  5  5  5  5  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  3\n  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  6  6  6  6  6  6  6  6  7\n  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9 10\n 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12  5\n  5  1  1  2  2  3  3  4  4  6  6  7  7  8  8  9  9 10 10 11 11 12 12]\n12\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 1. 0.]\n [0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 1.]]\nWARNING:tensorflow:From C:\\Apps\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n[[-9.4367e+00  1.1639e-01 -8.7374e-01 ...  1.7785e+01 -3.4744e+01\n   6.4241e+01]\n [-4.8660e+00  3.7209e-01 -6.6125e-01 ... -6.0199e-01 -2.4292e+01\n   5.6549e+01]\n [-9.7842e+00 -6.8761e-01  3.9075e-01 ...  8.9946e-01  1.7988e-01\n   7.3821e-01]\n ...\n [-2.1892e+01  1.1818e+00 -1.5830e+01 ...  7.2210e+01  5.7606e+01\n  -6.5558e+01]\n [-2.7137e+00 -1.1825e+00 -3.6538e+00 ... -1.3272e+01  1.9272e+01\n  -1.3692e+02]\n [-2.1772e+01 -3.3342e+00 -5.5211e+00 ... -1.9838e+01 -8.0757e+01\n   8.9580e+01]]\n[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
    }
   ],
   "source": [
    "# I tried removing 0 from classification seems to improve accuracy as expected 1.0. validation accuracy a little bit\n",
    "# but I haven't fully removed 13 and reduced to 12 classifications.\n",
    "# For some reason the one hot or the softmax model bit dont like it when I change the values form 13 to 12\n",
    "\n",
    "from helper_functions import mhealth_get_dataset\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Hyperparameters\n",
    "# This is how many samples per col\n",
    "data_length = 2\n",
    "\n",
    "# Prep data\n",
    "\n",
    "\n",
    "# Get the dataset\n",
    "dataset=mhealth_get_dataset()\n",
    "\n",
    "# shuffle dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "\n",
    "# training_users,test_data = dataset[:6], dataset[6:]\n",
    "# training_users,test_data = dataset[:8], dataset[8:] # get 8 training users and 2 test users\n",
    "training_users,test_data = dataset[:8], dataset[8:]\n",
    "\n",
    "\n",
    "# Remove all records with a certain value from the list of lists aka the 2D list under the 'data' key in each user dictionary in the list of user information dictionaries.\n",
    "def remove_records_by_val(lst_of_users_dicos, val=0):\n",
    "    for i in range(len(lst_of_users_dicos)):\n",
    "        lst_of_users_dicos[i]['data'] = [lst for lst in lst_of_users_dicos[i]['data'] if lst[23] != val]\n",
    "\n",
    "#Same thing just not as a function\n",
    "\n",
    "# for user in training_users:\n",
    "# for i in range(len(training_users)):\n",
    "#     training_users[i]['data'] = [lst for lst in training_users[i]['data'] if lst[23] != 0]\n",
    "\n",
    "# # for user in training_users:\n",
    "# for i in range(len(test_data)):\n",
    "#     test_data[i]['data'] = [lst for lst in test_data[i]['data'] if lst[23] != 0]\n",
    "\n",
    "\n",
    "# In this case removing 0 to see affect on validation accuracy\n",
    "# The 0 classification (no activity) is very... unpredictable and the network is most certainly having difficulty with this and other activities\n",
    "remove_records_by_val(training_users, 0)\n",
    "remove_records_by_val(test_data, 0)\n",
    "\n",
    "\n",
    "\n",
    "training_data_pre  = defaultdict(list) \n",
    "test_data_pre =defaultdict(list)\n",
    "\n",
    "counter=0\n",
    "previous_label=None\n",
    "to_put=[]\n",
    "for user in training_users:\n",
    "    for user_data in user['data']:\n",
    "        if not previous_label==user_data[23]:\n",
    "            counter=1\n",
    "            to_put=[]\n",
    "            previous_label=user_data[23]\n",
    "            user_data.pop()\n",
    "            to_put+=user_data\n",
    "        elif previous_label==user_data[23]:\n",
    "            counter+=1\n",
    "            user_data.pop()\n",
    "            to_put+=user_data\n",
    "        if counter == data_length:\n",
    "            training_data_pre[previous_label].append(to_put)\n",
    "            to_put=[]\n",
    "\n",
    "counter=0\n",
    "previous_label=None\n",
    "to_put=[]\n",
    "for user in test_data:\n",
    "    for user_data in user['data']:\n",
    "        if not previous_label==user_data[23]:\n",
    "            counter=1\n",
    "            to_put=[]\n",
    "            previous_label=user_data[23]\n",
    "            user_data.pop()\n",
    "            to_put+=user_data\n",
    "        elif previous_label==user_data[23]:\n",
    "            counter+=1\n",
    "            user_data.pop()\n",
    "            to_put+=user_data\n",
    "        if counter == data_length:\n",
    "            test_data_pre[previous_label].append(to_put)\n",
    "            to_put=[]\n",
    "\n",
    "training_data,training_labels=[],[]\n",
    "test_data,test_labels=[],[]\n",
    "\n",
    "def get_one_hot(targets, nb_classes): # 0 taken out means onehot is wrong. 1-12\n",
    "    print(targets)\n",
    "    print(nb_classes)\n",
    "    res = np.eye(nb_classes)[np.array(targets - 1).reshape(-1)]\n",
    "    # res = np.eye(nb_classes)[np.array(targets)]\n",
    "    print(res)\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "for training_label,training in training_data_pre.items():\n",
    "    for train in training:\n",
    "        training_labels.append(int(training_label))\n",
    "        training_data.append(train)\n",
    "\n",
    "training_data=np.array(training_data)\n",
    "\n",
    "\n",
    "for training_label,training in test_data_pre.items():\n",
    "    for train in training:\n",
    "        test_labels.append(int(training_label))\n",
    "        test_data.append(train)\n",
    "\n",
    "test_data=np.array(test_data)\n",
    "\n",
    "training_labels_len=len(training_labels)\n",
    "labels=training_labels+test_labels\n",
    "\n",
    "# removing 0 to see affect on validation accuracy\n",
    "# labels=get_one_hot(np.array(labels),13)\n",
    "labels=get_one_hot(np.array(labels),12)\n",
    "\n",
    "training_labels=labels[:training_labels_len]\n",
    "test_labels=labels[training_labels_len:]\n",
    "\n",
    "# Okay lets create our model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(data_length*23), # Our input layer\n",
    "    tf.keras.layers.Dense(2000,activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(2000),\n",
    "\n",
    "    # tf.keras.layers.Dense(13,activation=tf.keras.activations.softmax) # Our output layer we have 13 classifcations # removing 0 to see affect on validation accuracy\n",
    "    tf.keras.layers.Dense(12,activation=tf.keras.activations.softmax) # Without 0\n",
    "\n",
    "])\n",
    "# Compile our model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "print(test_data)\n",
    "\n",
    "print(test_labels)\n",
    "\n",
    "# model.fit(training_data,training_labels,validation_data=(test_data,test_labels),epochs=200)#100\n",
    "# -9.2305,3.9479,-4.0236,0.10884,-0.058608,3.4836,-7.467,-8.0885,0.24675,-0.78799,-0.21807,0.19791,18.714,5.4366,2.2399,-5.5648,7.5238,0.47255,-0.25051,1.0172,9.3079,-2.787,-15.309,9\n",
    "# -8.6677,3.5852,-4.5615,-0.054422,0.22606,-0.75304,-7.1983,-5.9716,0.24675,-0.78799,-0.21807,0.25394,7.7039,9.586,-0.92224,-5.7833,9.6692,0.47255,-0.25051,1.0172,4.6822,1.4321,-15.444,9\n",
    "\n",
    "# test_array = np.array([[-9.2305,3.9479,-4.0236,0.10884,-0.058608,3.4836,-7.467,-8.0885,0.24675,-0.78799,-0.21807,0.19791,18.714,5.4366,2.2399,-5.5648,7.5238,0.47255,-0.25051,1.0172,9.3079,-2.787,-15.309,-8.6677,3.5852,-4.5615,-0.054422,0.22606,-0.75304,-7.1983,-5.9716,0.24675,-0.78799,-0.21807,0.25394,7.7039,9.586,-0.92224,-5.7833,9.6692,0.47255,-0.25051,1.0172,4.6822,1.4321,-15.444]])\n",
    "\n",
    "# print(model.predict([test_array])[0][8]*100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# With 0 \n",
    "\n",
    "# Epoch 100/100\n",
    "\n",
    "# training_users,test_data = dataset[:6], dataset[6:] 6 training users 4 test\n",
    "#157/157 [==============================] - 0s 319us/sample - loss: 0.1574 - acc: 0.9108 - val_loss: 10.5625 - val_acc: 0.3402\n",
    "\n",
    "\n",
    "#   Without 0_______________________________________________________________________________________________________________________\n",
    "\n",
    "# 6 training users 4 test\n",
    "# Epoch 100/100\n",
    "# 71/71 [==============================] - 0s 479us/sample - loss: 0.0053 - acc: 1.0000 - val_loss: 21.2243 - val_acc: 0.5217\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Dense(data_length*23), # Our input layer\n",
    "#     tf.keras.layers.Dense(500,activation=tf.keras.activations.relu),\n",
    "#     tf.keras.layers.Dense(500),\n",
    "#     tf.keras.layers.Dense(12,activation=tf.keras.activations.softmax) # Without 0\n",
    "# ])\n",
    "\n",
    "# 8 training users 2 test\n",
    "#__________________________\n",
    "\n",
    "# With Denses 500 in middle\n",
    "# Epoch 200/200\n",
    "# 95/95 [==============================] - 0s 790us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 7.4909 - val_acc: 0.5833\n",
    "\n",
    "# With Denses in the middle 1000 instead of 500\n",
    "# Epoch 200/200\n",
    "# 94/94 [==============================] - 0s 373us/sample - loss: 4.2624e-04 - acc: 1.0000 - val_loss: 20.9304 - val_acc: 0.5833\n",
    "\n",
    "\n",
    "# With Denses in the middle 2000 instead of 500\n",
    "# Epoch 200/200\n",
    "# 95/95 [==============================] - 0s 717us/sample - loss: 2.9468e-04 - acc: 1.0000 - val_loss: 42.0160 - val_acc: 0.6250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          0        1         2         3         4         5        6   \\\n0   -9.43670  0.11639  -0.87374 -0.238620 -0.234430  -0.92331  -6.8344   \n1   -4.86600  0.37209  -0.66125  0.008373  0.154890   2.33750  -2.4792   \n2   -9.78420 -0.68761   0.39075 -0.644690 -0.192570  -0.93662  -9.4754   \n3   -9.72110 -0.81543  -0.90497 -0.037677 -0.142330   1.42520  -9.6110   \n4   -9.46510 -0.48207   3.38820 -0.221870 -0.113030   3.05560  -9.5139   \n5   -9.84210 -0.82369  -0.20045 -0.163270  0.004186   1.31170  -9.3219   \n6    0.56603 -2.03860   9.47970 -0.096285 -0.050235   3.91240   1.3665   \n7   -0.38150 -5.16740   8.28230 -0.171640 -0.138150   4.87880   1.1535   \n8   -8.23320 -0.46364  -1.26280 -0.159080 -0.075353   0.92177 -10.9150   \n9   -9.61210 -0.20378  -1.35520 -0.330720 -0.159080   2.50030  -9.3273   \n10  -5.87000  0.56918  -6.22250 -0.192570 -0.146520  -0.32793 -10.6290   \n11  -0.73736  8.58500  -6.71160 -0.133960 -0.117220   2.48710  -9.4806   \n12 -10.08400 -0.82758  -0.05343 -0.117220 -0.020931  -0.12014  -9.8553   \n13  -9.36910  0.27317   0.68774 -0.108840 -0.745160   1.12140  -9.8687   \n14 -10.20800 -1.22950  -1.39490  0.234430  0.104660   1.50190  -8.8355   \n15  -9.27360 -0.96455  -1.70790 -0.246990 -0.460490   1.71960  -9.4622   \n16  -7.08980  0.95636  -5.75600 -0.154890 -0.058608   1.80270 -11.9720   \n17  -7.06690  2.84020  -5.22860  0.146520  0.125590   1.72500 -10.7370   \n18   1.34550  1.40970  -2.25210  0.552590  0.213500  -4.05340 -18.5870   \n19  -1.94390 -0.78420   0.71890 -0.615380  0.334900   7.08110  -4.0552   \n20   6.76190  3.82230  -3.75070 -0.129770 -0.276300   8.03130  -9.1720   \n21 -21.89200  1.18180 -15.83000  0.380950  1.025600   1.13120 -19.0430   \n22  -2.71370 -1.18250  -3.65380  0.406070  0.799580  19.14400  -4.0837   \n23 -21.77200 -3.33420  -5.52110  0.000000 -0.544220  -5.98530  -5.2575   \n\n          7         8         9   ...         36        37         38  \\\n0   -3.24680 -0.519480 -0.418390  ... -19.476000  -2.52400  -9.573400   \n1   -1.38180  0.024119 -0.071295  ...  16.408000  -2.41220  -6.710200   \n2    1.96480 -0.493510 -0.559100  ...  -0.153490  -0.35746  -9.731700   \n3    1.27330  0.029685 -0.853660  ...  -0.146300  -2.95760  -8.934400   \n4   -0.89305  0.660480 -0.643530  ...  -0.007344  -1.52530  -7.390300   \n5    2.46370 -0.589980 -0.727950  ...  -0.292440  -8.67410  -3.776100   \n6    8.93040  0.469390  0.454030  ...  -0.148060  -4.67170  -1.594100   \n7    8.41900  0.428570  1.001900  ...  -0.146300  -6.90140   0.040333   \n8   -0.36985  0.324680 -0.523450  ...  -5.418300  -0.40356 -10.125000   \n9    1.92320  0.022263 -0.906190  ...  -0.024760  -2.38910  -8.747800   \n10   1.47570 -0.608530 -0.748590  ...   5.765200   2.72670  -8.846100   \n11   0.45709 -0.144710 -0.859290  ...  -4.695800  -2.04790 -11.705000   \n12   1.59980 -0.435990 -0.575990  ...  -0.171610   1.45810  -9.810000   \n13   1.24690 -0.298700 -0.863040  ...  -0.258340   0.19907   5.515300   \n14   2.58370 -0.502780 -0.786120  ...  -2.191300  -2.83240  -9.059000   \n15   2.29410 -0.196660 -0.874300  ...  -7.036700  -1.44140  -8.434700   \n16  -3.13270  0.385900 -0.771110  ...  -2.875800   4.51590  -6.978700   \n17   0.54289  0.022263 -0.748590  ...  -3.151600   1.23870  -5.866700   \n18   6.68640 -0.176250 -0.551590  ...   6.199400  -7.32620   1.736300   \n19 -18.39200  0.168830 -0.352720  ... -29.231000   6.03680  -7.198300   \n20 -15.22200  0.426720 -0.868670  ...   7.644000  -6.93060   6.820800   \n21  14.48600  0.155840 -0.545970  ...  33.117000 -21.15100  -0.896800   \n22 -18.27300 -0.597400 -0.771110  ...  10.774000   2.77120  -0.023658   \n23  -9.90610  0.397030 -0.718570  ... -36.527000  12.55600  -5.939000   \n\n         39        40        41        42        43        44          45  \n0   0.65675 -0.356860 -0.441480 -0.036638  17.78500 -34.74400   64.241000  \n1  -1.02300 -0.050980 -0.903490 -0.603450  -0.60199 -24.29200   56.549000  \n2   0.82591 -0.688240 -0.823410  0.241380   0.89946   0.17988    0.738210  \n3   1.56370  0.223530 -1.057500 -0.245690   0.36266   0.35939    0.003659  \n4   5.97420  0.331370 -0.860370  0.463360   0.53673  -0.18673    0.012602  \n5   2.39450 -0.986270  0.166320  0.017241   0.36074   0.16711   -1.077500  \n6   8.58700 -0.090196  0.394250  0.780170   0.36077   0.17072   -0.716510  \n7   7.03330 -0.364710  0.581110  0.864220   0.18481   0.52816   -1.445600  \n8   1.49370  0.056863 -0.661190  0.928880 -13.35500  28.92300   -6.693500  \n9   2.21890  0.062745 -1.069800 -0.265090 -28.78200  12.29800  -51.240000  \n10  1.39100 -0.417650 -0.876800 -0.385780  22.74900  30.75000   12.422000  \n11  0.49808 -0.219610 -0.905540 -0.510780  -1.21530 -13.80800  -20.825000  \n12  0.99908 -0.356860 -0.889120  0.515090   6.67640 -14.59000   -0.442590  \n13  7.52580 -0.554900  0.131420  0.952590 -11.13900  17.11900   12.963000  \n14  1.09910 -0.456860 -0.850100 -0.441810  -6.13820  14.54300  -21.929000  \n15  0.52618  0.194120 -0.944560 -0.528020 -12.28000  10.73000  -10.100000  \n16  5.13020  0.800000 -0.342920  0.799570  -1.16560   9.11280   20.824000  \n17  7.33170 -0.192160 -1.078000  0.265090   6.71470 -10.75800    1.685900  \n18  4.62850 -0.541180 -0.915810 -0.286640  33.91800  70.46500  -33.961000  \n19  0.17134 -0.809800  0.067762  0.784480  51.06300  25.49700  -67.104000  \n20  4.08060 -0.613730 -0.262830  0.887930  16.41000  43.12300  -63.280000  \n21 -7.68030 -0.252940 -0.938400 -0.443970  72.21000  57.60600  -65.558000  \n22 -4.36580 -0.884310  0.519510  0.362070 -13.27200  19.27200 -136.920000  \n23  1.09890 -0.727450 -0.195070  0.846980 -19.83800 -80.75700   89.580000  \n\n[24 rows x 46 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-9.43670</td>\n      <td>0.11639</td>\n      <td>-0.87374</td>\n      <td>-0.238620</td>\n      <td>-0.234430</td>\n      <td>-0.92331</td>\n      <td>-6.8344</td>\n      <td>-3.24680</td>\n      <td>-0.519480</td>\n      <td>-0.418390</td>\n      <td>...</td>\n      <td>-19.476000</td>\n      <td>-2.52400</td>\n      <td>-9.573400</td>\n      <td>0.65675</td>\n      <td>-0.356860</td>\n      <td>-0.441480</td>\n      <td>-0.036638</td>\n      <td>17.78500</td>\n      <td>-34.74400</td>\n      <td>64.241000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-4.86600</td>\n      <td>0.37209</td>\n      <td>-0.66125</td>\n      <td>0.008373</td>\n      <td>0.154890</td>\n      <td>2.33750</td>\n      <td>-2.4792</td>\n      <td>-1.38180</td>\n      <td>0.024119</td>\n      <td>-0.071295</td>\n      <td>...</td>\n      <td>16.408000</td>\n      <td>-2.41220</td>\n      <td>-6.710200</td>\n      <td>-1.02300</td>\n      <td>-0.050980</td>\n      <td>-0.903490</td>\n      <td>-0.603450</td>\n      <td>-0.60199</td>\n      <td>-24.29200</td>\n      <td>56.549000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-9.78420</td>\n      <td>-0.68761</td>\n      <td>0.39075</td>\n      <td>-0.644690</td>\n      <td>-0.192570</td>\n      <td>-0.93662</td>\n      <td>-9.4754</td>\n      <td>1.96480</td>\n      <td>-0.493510</td>\n      <td>-0.559100</td>\n      <td>...</td>\n      <td>-0.153490</td>\n      <td>-0.35746</td>\n      <td>-9.731700</td>\n      <td>0.82591</td>\n      <td>-0.688240</td>\n      <td>-0.823410</td>\n      <td>0.241380</td>\n      <td>0.89946</td>\n      <td>0.17988</td>\n      <td>0.738210</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-9.72110</td>\n      <td>-0.81543</td>\n      <td>-0.90497</td>\n      <td>-0.037677</td>\n      <td>-0.142330</td>\n      <td>1.42520</td>\n      <td>-9.6110</td>\n      <td>1.27330</td>\n      <td>0.029685</td>\n      <td>-0.853660</td>\n      <td>...</td>\n      <td>-0.146300</td>\n      <td>-2.95760</td>\n      <td>-8.934400</td>\n      <td>1.56370</td>\n      <td>0.223530</td>\n      <td>-1.057500</td>\n      <td>-0.245690</td>\n      <td>0.36266</td>\n      <td>0.35939</td>\n      <td>0.003659</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-9.46510</td>\n      <td>-0.48207</td>\n      <td>3.38820</td>\n      <td>-0.221870</td>\n      <td>-0.113030</td>\n      <td>3.05560</td>\n      <td>-9.5139</td>\n      <td>-0.89305</td>\n      <td>0.660480</td>\n      <td>-0.643530</td>\n      <td>...</td>\n      <td>-0.007344</td>\n      <td>-1.52530</td>\n      <td>-7.390300</td>\n      <td>5.97420</td>\n      <td>0.331370</td>\n      <td>-0.860370</td>\n      <td>0.463360</td>\n      <td>0.53673</td>\n      <td>-0.18673</td>\n      <td>0.012602</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-9.84210</td>\n      <td>-0.82369</td>\n      <td>-0.20045</td>\n      <td>-0.163270</td>\n      <td>0.004186</td>\n      <td>1.31170</td>\n      <td>-9.3219</td>\n      <td>2.46370</td>\n      <td>-0.589980</td>\n      <td>-0.727950</td>\n      <td>...</td>\n      <td>-0.292440</td>\n      <td>-8.67410</td>\n      <td>-3.776100</td>\n      <td>2.39450</td>\n      <td>-0.986270</td>\n      <td>0.166320</td>\n      <td>0.017241</td>\n      <td>0.36074</td>\n      <td>0.16711</td>\n      <td>-1.077500</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.56603</td>\n      <td>-2.03860</td>\n      <td>9.47970</td>\n      <td>-0.096285</td>\n      <td>-0.050235</td>\n      <td>3.91240</td>\n      <td>1.3665</td>\n      <td>8.93040</td>\n      <td>0.469390</td>\n      <td>0.454030</td>\n      <td>...</td>\n      <td>-0.148060</td>\n      <td>-4.67170</td>\n      <td>-1.594100</td>\n      <td>8.58700</td>\n      <td>-0.090196</td>\n      <td>0.394250</td>\n      <td>0.780170</td>\n      <td>0.36077</td>\n      <td>0.17072</td>\n      <td>-0.716510</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.38150</td>\n      <td>-5.16740</td>\n      <td>8.28230</td>\n      <td>-0.171640</td>\n      <td>-0.138150</td>\n      <td>4.87880</td>\n      <td>1.1535</td>\n      <td>8.41900</td>\n      <td>0.428570</td>\n      <td>1.001900</td>\n      <td>...</td>\n      <td>-0.146300</td>\n      <td>-6.90140</td>\n      <td>0.040333</td>\n      <td>7.03330</td>\n      <td>-0.364710</td>\n      <td>0.581110</td>\n      <td>0.864220</td>\n      <td>0.18481</td>\n      <td>0.52816</td>\n      <td>-1.445600</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-8.23320</td>\n      <td>-0.46364</td>\n      <td>-1.26280</td>\n      <td>-0.159080</td>\n      <td>-0.075353</td>\n      <td>0.92177</td>\n      <td>-10.9150</td>\n      <td>-0.36985</td>\n      <td>0.324680</td>\n      <td>-0.523450</td>\n      <td>...</td>\n      <td>-5.418300</td>\n      <td>-0.40356</td>\n      <td>-10.125000</td>\n      <td>1.49370</td>\n      <td>0.056863</td>\n      <td>-0.661190</td>\n      <td>0.928880</td>\n      <td>-13.35500</td>\n      <td>28.92300</td>\n      <td>-6.693500</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-9.61210</td>\n      <td>-0.20378</td>\n      <td>-1.35520</td>\n      <td>-0.330720</td>\n      <td>-0.159080</td>\n      <td>2.50030</td>\n      <td>-9.3273</td>\n      <td>1.92320</td>\n      <td>0.022263</td>\n      <td>-0.906190</td>\n      <td>...</td>\n      <td>-0.024760</td>\n      <td>-2.38910</td>\n      <td>-8.747800</td>\n      <td>2.21890</td>\n      <td>0.062745</td>\n      <td>-1.069800</td>\n      <td>-0.265090</td>\n      <td>-28.78200</td>\n      <td>12.29800</td>\n      <td>-51.240000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-5.87000</td>\n      <td>0.56918</td>\n      <td>-6.22250</td>\n      <td>-0.192570</td>\n      <td>-0.146520</td>\n      <td>-0.32793</td>\n      <td>-10.6290</td>\n      <td>1.47570</td>\n      <td>-0.608530</td>\n      <td>-0.748590</td>\n      <td>...</td>\n      <td>5.765200</td>\n      <td>2.72670</td>\n      <td>-8.846100</td>\n      <td>1.39100</td>\n      <td>-0.417650</td>\n      <td>-0.876800</td>\n      <td>-0.385780</td>\n      <td>22.74900</td>\n      <td>30.75000</td>\n      <td>12.422000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-0.73736</td>\n      <td>8.58500</td>\n      <td>-6.71160</td>\n      <td>-0.133960</td>\n      <td>-0.117220</td>\n      <td>2.48710</td>\n      <td>-9.4806</td>\n      <td>0.45709</td>\n      <td>-0.144710</td>\n      <td>-0.859290</td>\n      <td>...</td>\n      <td>-4.695800</td>\n      <td>-2.04790</td>\n      <td>-11.705000</td>\n      <td>0.49808</td>\n      <td>-0.219610</td>\n      <td>-0.905540</td>\n      <td>-0.510780</td>\n      <td>-1.21530</td>\n      <td>-13.80800</td>\n      <td>-20.825000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-10.08400</td>\n      <td>-0.82758</td>\n      <td>-0.05343</td>\n      <td>-0.117220</td>\n      <td>-0.020931</td>\n      <td>-0.12014</td>\n      <td>-9.8553</td>\n      <td>1.59980</td>\n      <td>-0.435990</td>\n      <td>-0.575990</td>\n      <td>...</td>\n      <td>-0.171610</td>\n      <td>1.45810</td>\n      <td>-9.810000</td>\n      <td>0.99908</td>\n      <td>-0.356860</td>\n      <td>-0.889120</td>\n      <td>0.515090</td>\n      <td>6.67640</td>\n      <td>-14.59000</td>\n      <td>-0.442590</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-9.36910</td>\n      <td>0.27317</td>\n      <td>0.68774</td>\n      <td>-0.108840</td>\n      <td>-0.745160</td>\n      <td>1.12140</td>\n      <td>-9.8687</td>\n      <td>1.24690</td>\n      <td>-0.298700</td>\n      <td>-0.863040</td>\n      <td>...</td>\n      <td>-0.258340</td>\n      <td>0.19907</td>\n      <td>5.515300</td>\n      <td>7.52580</td>\n      <td>-0.554900</td>\n      <td>0.131420</td>\n      <td>0.952590</td>\n      <td>-11.13900</td>\n      <td>17.11900</td>\n      <td>12.963000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-10.20800</td>\n      <td>-1.22950</td>\n      <td>-1.39490</td>\n      <td>0.234430</td>\n      <td>0.104660</td>\n      <td>1.50190</td>\n      <td>-8.8355</td>\n      <td>2.58370</td>\n      <td>-0.502780</td>\n      <td>-0.786120</td>\n      <td>...</td>\n      <td>-2.191300</td>\n      <td>-2.83240</td>\n      <td>-9.059000</td>\n      <td>1.09910</td>\n      <td>-0.456860</td>\n      <td>-0.850100</td>\n      <td>-0.441810</td>\n      <td>-6.13820</td>\n      <td>14.54300</td>\n      <td>-21.929000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-9.27360</td>\n      <td>-0.96455</td>\n      <td>-1.70790</td>\n      <td>-0.246990</td>\n      <td>-0.460490</td>\n      <td>1.71960</td>\n      <td>-9.4622</td>\n      <td>2.29410</td>\n      <td>-0.196660</td>\n      <td>-0.874300</td>\n      <td>...</td>\n      <td>-7.036700</td>\n      <td>-1.44140</td>\n      <td>-8.434700</td>\n      <td>0.52618</td>\n      <td>0.194120</td>\n      <td>-0.944560</td>\n      <td>-0.528020</td>\n      <td>-12.28000</td>\n      <td>10.73000</td>\n      <td>-10.100000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-7.08980</td>\n      <td>0.95636</td>\n      <td>-5.75600</td>\n      <td>-0.154890</td>\n      <td>-0.058608</td>\n      <td>1.80270</td>\n      <td>-11.9720</td>\n      <td>-3.13270</td>\n      <td>0.385900</td>\n      <td>-0.771110</td>\n      <td>...</td>\n      <td>-2.875800</td>\n      <td>4.51590</td>\n      <td>-6.978700</td>\n      <td>5.13020</td>\n      <td>0.800000</td>\n      <td>-0.342920</td>\n      <td>0.799570</td>\n      <td>-1.16560</td>\n      <td>9.11280</td>\n      <td>20.824000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-7.06690</td>\n      <td>2.84020</td>\n      <td>-5.22860</td>\n      <td>0.146520</td>\n      <td>0.125590</td>\n      <td>1.72500</td>\n      <td>-10.7370</td>\n      <td>0.54289</td>\n      <td>0.022263</td>\n      <td>-0.748590</td>\n      <td>...</td>\n      <td>-3.151600</td>\n      <td>1.23870</td>\n      <td>-5.866700</td>\n      <td>7.33170</td>\n      <td>-0.192160</td>\n      <td>-1.078000</td>\n      <td>0.265090</td>\n      <td>6.71470</td>\n      <td>-10.75800</td>\n      <td>1.685900</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1.34550</td>\n      <td>1.40970</td>\n      <td>-2.25210</td>\n      <td>0.552590</td>\n      <td>0.213500</td>\n      <td>-4.05340</td>\n      <td>-18.5870</td>\n      <td>6.68640</td>\n      <td>-0.176250</td>\n      <td>-0.551590</td>\n      <td>...</td>\n      <td>6.199400</td>\n      <td>-7.32620</td>\n      <td>1.736300</td>\n      <td>4.62850</td>\n      <td>-0.541180</td>\n      <td>-0.915810</td>\n      <td>-0.286640</td>\n      <td>33.91800</td>\n      <td>70.46500</td>\n      <td>-33.961000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-1.94390</td>\n      <td>-0.78420</td>\n      <td>0.71890</td>\n      <td>-0.615380</td>\n      <td>0.334900</td>\n      <td>7.08110</td>\n      <td>-4.0552</td>\n      <td>-18.39200</td>\n      <td>0.168830</td>\n      <td>-0.352720</td>\n      <td>...</td>\n      <td>-29.231000</td>\n      <td>6.03680</td>\n      <td>-7.198300</td>\n      <td>0.17134</td>\n      <td>-0.809800</td>\n      <td>0.067762</td>\n      <td>0.784480</td>\n      <td>51.06300</td>\n      <td>25.49700</td>\n      <td>-67.104000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>6.76190</td>\n      <td>3.82230</td>\n      <td>-3.75070</td>\n      <td>-0.129770</td>\n      <td>-0.276300</td>\n      <td>8.03130</td>\n      <td>-9.1720</td>\n      <td>-15.22200</td>\n      <td>0.426720</td>\n      <td>-0.868670</td>\n      <td>...</td>\n      <td>7.644000</td>\n      <td>-6.93060</td>\n      <td>6.820800</td>\n      <td>4.08060</td>\n      <td>-0.613730</td>\n      <td>-0.262830</td>\n      <td>0.887930</td>\n      <td>16.41000</td>\n      <td>43.12300</td>\n      <td>-63.280000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>-21.89200</td>\n      <td>1.18180</td>\n      <td>-15.83000</td>\n      <td>0.380950</td>\n      <td>1.025600</td>\n      <td>1.13120</td>\n      <td>-19.0430</td>\n      <td>14.48600</td>\n      <td>0.155840</td>\n      <td>-0.545970</td>\n      <td>...</td>\n      <td>33.117000</td>\n      <td>-21.15100</td>\n      <td>-0.896800</td>\n      <td>-7.68030</td>\n      <td>-0.252940</td>\n      <td>-0.938400</td>\n      <td>-0.443970</td>\n      <td>72.21000</td>\n      <td>57.60600</td>\n      <td>-65.558000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>-2.71370</td>\n      <td>-1.18250</td>\n      <td>-3.65380</td>\n      <td>0.406070</td>\n      <td>0.799580</td>\n      <td>19.14400</td>\n      <td>-4.0837</td>\n      <td>-18.27300</td>\n      <td>-0.597400</td>\n      <td>-0.771110</td>\n      <td>...</td>\n      <td>10.774000</td>\n      <td>2.77120</td>\n      <td>-0.023658</td>\n      <td>-4.36580</td>\n      <td>-0.884310</td>\n      <td>0.519510</td>\n      <td>0.362070</td>\n      <td>-13.27200</td>\n      <td>19.27200</td>\n      <td>-136.920000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>-21.77200</td>\n      <td>-3.33420</td>\n      <td>-5.52110</td>\n      <td>0.000000</td>\n      <td>-0.544220</td>\n      <td>-5.98530</td>\n      <td>-5.2575</td>\n      <td>-9.90610</td>\n      <td>0.397030</td>\n      <td>-0.718570</td>\n      <td>...</td>\n      <td>-36.527000</td>\n      <td>12.55600</td>\n      <td>-5.939000</td>\n      <td>1.09890</td>\n      <td>-0.727450</td>\n      <td>-0.195070</td>\n      <td>0.846980</td>\n      <td>-19.83800</td>\n      <td>-80.75700</td>\n      <td>89.580000</td>\n    </tr>\n  </tbody>\n</table>\n<p>24 rows × 46 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# df"
   ]
  }
 ]
}